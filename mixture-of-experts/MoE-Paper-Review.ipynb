{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3e5472",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3bbfa0; font-size:2em;\">*Peering Behind the Iron Curtain of the LLM Cold War:*</span> <span style=\"color:lightgray; font-size:2em;\">*Minor Leaks Reveal Some Insights into GPT-4's Evolution*</span>\n",
    "\n",
    "<img src=\"https://i.ibb.co/t3Z8PxB/Picture1.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0449a134",
   "metadata": {},
   "source": [
    "<span style=\"color:gray; font-size:0.8em;\">The comparison of OpenAI to the USSR is sensational, but it's intended to underscore a particular aspect: the quiet yet intense competition among major corporations. This rivalry, aimed at protecting their respective advancements, mirrors the silent standoff that characterized the nuclear arms race during the Cold War.</span>\n",
    "\n",
    "\n",
    "### <span style=\"color:#3bbfa0; font-size:1.2em;\">*1. Background of the MoE Leak:*</span>\n",
    "___\n",
    "\n",
    "Yam Peleg leaked unconfirmed third party information about GPT-4 onto twitter. Yam claimed in his now removed post <span style=\"color:red\">**“it is over. Everything is here:”**</span>\n",
    ". Which is likely to cause a skeptic’s “Hyperbolic alt-news salesman” alarms to ring. Nevertheless, let's examine these leaks.\n",
    "\n",
    "\n",
    "<img src=\"https://i.ibb.co/LhzQSDq/Picture2.png\" width=\"600\">\n",
    "\n",
    "<div style=\"position: relative; width: 700px;\">\n",
    "    <img src=\"./assets/img/stan.gif\" style=\"position: absolute; bottom: 70px; right: 0px; width: 130px;\">\n",
    "</div>\n",
    "\n",
    "#### 1.1 Leak Overview\n",
    "First order of business, let's get the mundane filler points out of the way without belaboring them excessively:\n",
    "\n",
    "- **Model Size**: GPT-4 is a behemoth, boasting about 1.8 trillion parameters across 120 to 128 layers. It's more than 10 times the size of its predecessor, GPT-3, marking a significant leap in the evolution of AI models.\n",
    "- **Training Data**: GPT-4 was trained on approximately 13 trillion tokens (not unique), with two epochs for text-based data and four for code-based data. Fine-tuning data was sourced from Scale AI and OpenAI's internal datasets.\n",
    "- **Cost**: The estimated cost of training GPT-4 in the cloud is around 63 million dollars, considering that an A100 costs about 1 dollar per hour.\n",
    "- <span style=\"color:blue;\">**Legality**</span>: There is also some buzz about OpenAI using a lot of Textbooks as training data. I believe they are adopting a legal version of “better to ask for forgiveness than permission”. Or perhaps more fittingly,”it's better to pay the fine than the licensing because by the time the lawsuits roll in, we'll have achieved AGI.” I admire the audacious approach in an era where clear-cut laws are still few and far between.\n",
    "\n",
    "\n",
    "Does any of that strike you as groundbreaking or significant enough to declare 'it's over'? I do not think so!\n",
    "\n",
    "\n",
    "However, there is one really interesting revelation here which is OpenAI’s use of Mixture of Experts (MoE). This approach has been used by Meta AI and Google AI for both language and vision models. MoE is a type of conditional computation where parts of the network are activated on a per-example basis. This approach increases model capacity without a proportional increase in computation. However, a poor expert routing strategy can lead to under-training of certain experts.\n",
    "\n",
    "The idea is quite simple. Rather than combining multiple models in the usual manner, a router is used to decide which model should receive a particular sample for prediction. And this isn't exactly a new concept:\n",
    "\n",
    "<img src=\"https://i.ibb.co/nmDnSwZ/Picture3.png\" width=\"600\">\n",
    "\n",
    "However this report indicated that 16 different experts were used. This is likely a tuned parameter (Yikes).\n",
    "If the reports are indeed accurate and GPT-4 is built in this manner, it's certainly intriguing. This is undoubtedly information that OpenAI would prefer to keep confidential. However, it is laughable to claim “It is over. Everything is here”. Most of what adversarial companies would want to know are details about the routing methodology. Moreover, it's probable that most major competitors had already assumed this technique was being used, given its ability to accelerate the training of “Outrageously Large Networks”. Regardless, the MoE approach is quite fascinating. I've provided more details on the subject below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f85eef",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3bbfa0; font-size:1.2em;\">*2. Literature Review (With Code):*</span>\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4610cab",
   "metadata": {},
   "source": [
    "https://doi.org/10.48550/arXiv.2208.02813\n",
    "\n",
    "<img src=\"./assets/img/PaperAbs.png\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f93a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e37a21",
   "metadata": {},
   "source": [
    "<img src=\"./assets/img/MoEDemo.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ff6b0",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3bbfa0; font-size:1.2em;\">*2.1 Basic Mixture of Experts (MoE) Models*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d1cc3",
   "metadata": {},
   "source": [
    "This paper covers the evolution of the Mixture of Experts (MoE) methods:\n",
    "\n",
    "- **SimpleMoE**: Jordan, M. I. and Jacobs, R. A. (1994). Hierarchical mixtures of experts and the em algorithm. Neural computation 6 181–214. https://www.cs.toronto.edu/~hinton/absps/hme.pdf\n",
    "- **SparseMoE**: Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G. and Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 \n",
    "https://doi.org/10.48550/arXiv.1701.06538\n",
    "- **SingleMoE**: Fedus, W., Zoph, B. and Shazeer, N. (2021). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961 \n",
    "https://doi.org/10.48550/arXiv.2101.03961\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e8b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Where each expert is chosen to be a two-layer CNN\n",
    "\"\"\"\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(Expert, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(128*32*32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69940e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SimpleMoE: Jordan, M. I. and Jacobs, R. A. (1994). Hierarchical mixtures of experts and the em algorithm. \n",
    "Neural computation 6 181–214. https://www.cs.toronto.edu/~hinton/absps/hme.pdf\n",
    "\"\"\"\n",
    "class SimpleMoE(nn.Module):\n",
    "    def __init__(self, num_experts, input_channels, output_channels):\n",
    "        super(SimpleMoE, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_channels, output_channels) for _ in range(num_experts)])\n",
    "        self.gating_network = nn.Sequential(\n",
    "            nn.Flatten(),  # Add this line\n",
    "            nn.Linear(input_channels*32*32, num_experts),  # Modify this line\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = self.gating_network(x)\n",
    "        outputs = torch.stack([expert(x) * weight.unsqueeze(0) for expert, weight in zip(self.experts, weights)])\n",
    "        return outputs.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c904e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SparseMoE: Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G. and Dean, J. (2017). \n",
    "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint \n",
    "arXiv:1701.06538 https://doi.org/10.48550/arXiv.1701.06538\n",
    "\"\"\"\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, num_experts, input_channels, output_channels, fraction_experts=0.1):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_channels, output_channels) for _ in range(num_experts)])\n",
    "        self.gating_network = nn.Sequential(\n",
    "            nn.Flatten(),  # Add this line\n",
    "            nn.Linear(input_channels*32*32, num_experts),  # Modify this line\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.num_experts = num_experts\n",
    "        self.fraction_experts = fraction_experts\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = self.gating_network(x)\n",
    "        # Get the top k experts\n",
    "        top_weights, top_indices = torch.topk(weights, k=int(self.num_experts * self.fraction_experts))  # Select experts\n",
    "        outputs = torch.stack([self.experts[idx](x) * weight for idx, weight in zip(top_indices, top_weights)])\n",
    "        return outputs.sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d5bc8",
   "metadata": {},
   "source": [
    "___\n",
    "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbsjq4iseHi-Azxcj0irBjGkma0yd4geSPPombnJSdd5dyzTguUU2pdFfZu4G38G4F4TiymUOaIkQnXGVAix5x8wF3-9Ov3NJwWaEZNvJY84CWCgU5MbUYI_DjKa_BvalTHu3eyfCJGR89UqwskKngsppDy94Gahz3HAoKLh2vmh-Jzb7ZedRI91OwFw/w640-h339/image1.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9662f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SingleMoE: Fedus, W., Zoph, B. and Shazeer, N. (2021). Switch transformers: Scaling to trillion parameter models\n",
    "with simple and efficient sparsity. arXiv preprint arXiv:2101.03961 https://doi.org/10.48550/arXiv.2101.03961\n",
    "\"\"\"\n",
    "class SingleMoE(nn.Module):\n",
    "    def __init__(self, num_experts, input_channels, output_channels, topk=1):\n",
    "        super(SingleMoE, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_channels, output_channels) for _ in range(num_experts)])\n",
    "        self.gating_network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_channels*32*32, num_experts),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.num_experts = num_experts\n",
    "        self.topk = topk\n",
    "        self.expert_inputs = [[] for _ in range(num_experts)]  # Store inputs for each expert\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = self.gating_network(x)\n",
    "        # Get the top k experts\n",
    "        top_weights, top_indices = torch.topk(weights, k=self.topk, dim=1)\n",
    "        \n",
    "        outputs = []\n",
    "        for i in range(x.shape[0]):  # For each item in the batch\n",
    "            expert_outputs = []\n",
    "            for idx in top_indices[i]:\n",
    "                self.expert_inputs[idx].append(x[i].detach().cpu().numpy())  # Log inputs\n",
    "                expert_outputs.append(self.experts[idx](x[i].unsqueeze(0)))\n",
    "            expert_outputs = torch.stack(expert_outputs)\n",
    "            outputs.append((top_weights[i].unsqueeze(-1) * expert_outputs).sum(dim=0))  # Weighted sum of top-k expert outputs\n",
    "        \n",
    "        return torch.cat(outputs, dim=0)\n",
    "    \n",
    "\"\"\"\n",
    "The softmax operation in the gating network is used to convert the raw scores from the gating network into a \n",
    "probability distribution over the experts. This distribution represents the \"confidence\" of the gating network \n",
    "in each expert for the given input.\n",
    "\n",
    "When the output of the selected expert is multiplied by the corresponding softmax score, it's a way of weighting\n",
    "the output of the expert by the confidence of the gating network in that expert. This can be interpreted as a \n",
    "form of \"attention\" mechanism, where the model pays more attention to the outputs of the experts that the gating \n",
    "network is more confident in.\n",
    "\"\"\"\n",
    "class SingleAttnMoE(nn.Module):\n",
    "    def __init__(self, num_experts, input_channels, output_channels):\n",
    "        super(SingleAttnMoE, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_channels, output_channels) for _ in range(num_experts)])\n",
    "        self.gating_network = nn.Sequential(\n",
    "            nn.Flatten(),  # Add this line\n",
    "            nn.Linear(input_channels*32*32, num_experts),  # Modify this line\n",
    "        )\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = self.gating_network(x)\n",
    "        weights = F.softmax(weights, dim=1)  # Convert raw scores to probabilities\n",
    "        _, top_indices = torch.topk(weights, k=1)  # Select top 1 expert\n",
    "        expert = self.experts[top_indices[0]]\n",
    "        output = expert(x) * weights[0, top_indices[0]]  # Weight output by softmax score\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf356f",
   "metadata": {},
   "source": [
    "Load the data\n",
    "<img src=\"https://production-media.paperswithcode.com/datasets/4fdf2b82-2bc3-4f97-ba51-400322b228b1.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89333ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16baee0a",
   "metadata": {},
   "source": [
    "Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1996b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "num_experts = 10\n",
    "input_channels = 3  # CIFAR-10 images have 3 color channels\n",
    "output_channels = 10  # CIFAR-10 has 10 classes\n",
    "model = SingleMoE(num_experts, input_channels, output_channels).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebda816",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd7f2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i%2000==0:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss/2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad66b42",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ibb.co/xDXVkH8/Capture-Mo-E.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a70f5",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "### <span style=\"color:#3bbfa0; font-size:1.2em;\">*2.2 Novel Mixture of Experts (MoE) Improvements*</span>\n",
    "___\n",
    "<span style=\"color:#3bbfa0; font-size:1.3em;\">**2.2.1. Expert Diversification and Dispatching (EDD)**</span><br>\n",
    "<span style=\"color:gray; font-size:1.2em;\">*The authors propose the Expert Diversification and Dispatching (EDD) method, which fosters diversity among experts and optimizes data routing by introducing additional loss terms, thereby improving the overall performance of the Mixture of Experts model.*</span>\n",
    "<br><br>\n",
    "<span style=\"color:#3bbfa0; font-size:1.3em;\">**2.2.2. Stability by Smoothing**</span><br>\n",
    "<span style=\"color:gray; font-size:1.2em;\">*The authors introduce a noise term to the gating network output, which ensures a smooth transition between different routing behaviors and makes the router more stable.*</span>\n",
    "___\n",
    "<br><br><br>\n",
    "<span style=\"color:#3bbfa0; font-size:1.4em;\">**2.2.1 Expert Diversification and Dispatching (EDD)**</span><br>\n",
    "\n",
    "- **Introduction**: The section opens with an explanation about the behavior of Mixture of Experts (MoE) models. Unlike a single model, experts in a MoE model tend to diversify. The reason behind this behavior lies in the MoE model's unique design and training process. Each expert in the model is designed to specialize in a particular segment of the input space, fostering diversity and preventing the experts from collapsing into a single model.\n",
    "\n",
    "- **Model Architecture and Training**: The text then describes how a component in the MoE model called the router functions. The router's job is to allocate data to the appropriate expert through a learning process. It employs a gating network that uses a softmax function to assign probabilities to each expert. The gating network is then trained via backpropagation and gradient descent, akin to other elements of the neural network.\n",
    "\n",
    "- **Expert Diversification and Dispatching (EDD)**: The document introduces a novel training methodology for MoE models. This technique, termed \"Expert Diversification and Dispatching (EDD)\", promotes expert diversification. It accomplishes this by introducing an additional loss term, which penalizes the similarity between outputs from different experts, thereby encouraging each expert to specialize in unique parts of the input space.\n",
    "\n",
    "- **Advantages of EDD**: The EDD method also enhances the router's efficiency in dispatching data to the correct expert. It incorporates a dispatching loss that compels the router to allocate each data point to the expert most likely to yield the correct output. This is accomplished by comparing each expert's output with the target output and penalizing the router if it does not give the highest probability to the expert with the closest output to the target. This technique improves the router's proficiency at mapping inputs to experts, thereby enhancing the MoE model's overall performance.\n",
    "\n",
    "- **Conclusion**: The document concludes that despite the potential complexity, the EDD method aids in creating more diverse experts within a MoE model and improves its overall performance. The text implies that future research could further optimize this model by making the dispatching process more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d5bc2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoeEddLoss(nn.Module):\n",
    "    def __init__(self, num_experts, input_channels, output_channels, topk=1, num_samples=10, noise_stddev=0.1):\n",
    "        super(SparseMoeEddLoss, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_channels, output_channels) for _ in range(num_experts)])\n",
    "        self.gating_network = nn.Sequential(\n",
    "            nn.Linear(input_channels*32*32, num_experts),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.num_experts = num_experts\n",
    "        self.topk = topk\n",
    "        self.expert_inputs = [[] for _ in range(num_experts)]  # Store inputs for each expert\n",
    "        self.num_samples = num_samples  # Number of noisy samples to use for smoothing\n",
    "        self.noise_stddev = noise_stddev  # Standard deviation of the Gaussian noise\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(x.size(0), -1)  # Flatten the input\n",
    "        weights = self.gating_network(x_flat)\n",
    "        top_weights, top_indices = torch.topk(weights, k=self.topk, dim=1)  # Get the top k experts\n",
    "\n",
    "        outputs = []\n",
    "        averaged_noisy_outputs_list = []\n",
    "        for i in range(x.shape[0]):  # For each item in the batch\n",
    "            averaged_noisy_outputs = []\n",
    "            for idx in top_indices[i]:\n",
    "                self.expert_inputs[idx].append(x[i].detach().cpu().numpy())  # Log inputs\n",
    "\n",
    "                # Add noise to the input and average over multiple noisy samples\n",
    "                noisy_outputs = []\n",
    "                for _ in range(self.num_samples):\n",
    "                    noise = torch.randn_like(x[i]) * self.noise_stddev\n",
    "                    noisy_output = self.experts[idx]((x[i] + noise).unsqueeze(0))\n",
    "                    noisy_outputs.append(noisy_output)\n",
    "                averaged_noisy_outputs.append(torch.stack(noisy_outputs).mean(dim=0))\n",
    "\n",
    "            averaged_noisy_outputs = torch.stack(averaged_noisy_outputs)\n",
    "            averaged_noisy_outputs_list.append(averaged_noisy_outputs)\n",
    "            outputs.append((top_weights[i].unsqueeze(-1) * averaged_noisy_outputs).sum(dim=0))  # Weighted sum of top-k expert outputs\n",
    "\n",
    "        return torch.cat(outputs, dim=0), averaged_noisy_outputs_list, top_indices\n",
    "\n",
    "\n",
    "    def edd_loss(self, x, target, alpha=0.5):\n",
    "        x_flat = x.view(x.size(0), -1)  # Flatten the input\n",
    "        weights = self.gating_network(x_flat)\n",
    "        top_weights, top_indices = torch.topk(weights, k=self.topk, dim=1)  # Get the top k experts\n",
    "\n",
    "        # Diversification loss\n",
    "        outputs = []\n",
    "        for i in range(x.shape[0]):  # For each item in the batch\n",
    "            expert_outputs = []\n",
    "            for idx in top_indices[i]:\n",
    "                expert_outputs.append(self.experts[idx](x[i].unsqueeze(0)))\n",
    "            expert_outputs = torch.stack(expert_outputs)\n",
    "            outputs.append((top_weights[i].unsqueeze(-1) * expert_outputs).sum(dim=0))  # Weighted sum of top-k expert outputs\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "        diversification_loss = ((outputs - outputs.mean(dim=0, keepdim=True))**2).sum()\n",
    "\n",
    "        # Dispatching loss\n",
    "        dispatching_loss = 0\n",
    "        for i in range(x.shape[0]):  # For each item in the batch\n",
    "            expert_outputs = []\n",
    "            for idx in top_indices[i]:\n",
    "                expert_output = self.experts[idx](x[i].unsqueeze(0))\n",
    "                expert_outputs.append(expert_output)\n",
    "            expert_outputs = torch.stack(expert_outputs)\n",
    "            target_output = target[i].unsqueeze(0).expand_as(expert_outputs)\n",
    "            dispatching_loss += (top_weights[i].unsqueeze(-1) * (expert_outputs - target_output)**2).sum()\n",
    "\n",
    "        dispatching_loss = dispatching_loss / x.shape[0]  # Average over the batch size\n",
    "\n",
    "        return alpha * diversification_loss + (1-alpha) * dispatching_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1389e20",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:#3bbfa0; font-size:1.4em;\">**2.2.2 Stability by Smoothing**</span><br>\n",
    "\n",
    "- **Introduction**: The section begins by stating that the stability of a model can be improved by smoothing the model's predictions. The idea is to make the model's output less sensitive to small changes in the input. This is particularly important in machine learning models that are used for decision-making, where a small change in the input should not drastically change the model's decision.\n",
    "\n",
    "- **Smoothing Method**: The document then describes a specific method of smoothing. This method involves adding a small amount of random noise to the input data and then averaging the model's predictions over many noisy versions of the input. This process is known as Monte Carlo integration. The noise is added according to a Gaussian distribution, which is a common choice for this kind of operation.\n",
    "\n",
    "- **Advantages**: The document mentions that this smoothing method has several advantages. It can improve the model's stability and robustness, making it less likely to make drastically different predictions for similar inputs. It can also help to prevent overfitting, where the model learns to perform very well on the training data but poorly on new, unseen data.\n",
    "\n",
    "- **Disadvantages**: However, the document also notes that this smoothing method can be computationally expensive. This is because it requires the model to make predictions for many different versions of each input. This can be mitigated by using a smaller number of noisy versions, but this may also reduce the effectiveness of the smoothing.\n",
    "\n",
    "- **Conclusion**: The document concludes by stating that despite the computational cost, smoothing can be a valuable tool for improving the stability of machine learning models. It suggests that future research could explore ways to make the smoothing process more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c94ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoeEddLossStabSmoothing(nn.Module):\n",
    "    def __init__(self, num_experts, input_channels, output_channels, topk=1, num_samples=10, noise_stddev=0.1):\n",
    "        super(SparseMoeEddLoss, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_channels, output_channels) for _ in range(num_experts)])\n",
    "        self.gating_network = nn.Sequential(\n",
    "            nn.Linear(input_channels*32*32, num_experts),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.num_experts = num_experts\n",
    "        self.topk = topk\n",
    "        self.expert_inputs = [[] for _ in range(num_experts)]  # Store inputs for each expert\n",
    "        self.num_samples = num_samples  # Number of noisy samples to use for smoothing\n",
    "        self.noise_stddev = noise_stddev  # Standard deviation of the Gaussian noise\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(x.size(0), -1)  # Flatten the input\n",
    "        weights = self.gating_network(x_flat)\n",
    "        top_weights, top_indices = torch.topk(weights, k=self.topk, dim=1)  # Get the top k experts\n",
    "\n",
    "        outputs = []\n",
    "        averaged_noisy_outputs_list = []\n",
    "        for i in range(x.shape[0]):  # For each item in the batch\n",
    "            averaged_noisy_outputs = []\n",
    "            for idx in top_indices[i]:\n",
    "                self.expert_inputs[idx].append(x[i].detach().cpu().numpy())  # Log inputs\n",
    "\n",
    "                # Add noise to the input and average over multiple noisy samples\n",
    "                noisy_outputs = []\n",
    "                for _ in range(self.num_samples):\n",
    "                    noise = torch.randn_like(x[i]) * self.noise_stddev\n",
    "                    noisy_output = self.experts[idx]((x[i] + noise).unsqueeze(0))\n",
    "                    noisy_outputs.append(noisy_output)\n",
    "                averaged_noisy_outputs.append(torch.stack(noisy_outputs).mean(dim=0))\n",
    "\n",
    "            averaged_noisy_outputs = torch.stack(averaged_noisy_outputs)\n",
    "            averaged_noisy_outputs_list.append(averaged_noisy_outputs)\n",
    "            outputs.append((top_weights[i].unsqueeze(-1) * averaged_noisy_outputs).sum(dim=0))  # Weighted sum of top-k expert outputs\n",
    "\n",
    "        return torch.cat(outputs, dim=0), averaged_noisy_outputs_list, top_indices\n",
    "\n",
    "\n",
    "    def edd_loss(self, x, target, alpha=0.5):\n",
    "        x_flat = x.view(x.size(0), -1)  # Flatten the input\n",
    "        weights = self.gating_network(x_flat)\n",
    "        top_weights, top_indices = torch.topk(weights, k=self.topk, dim=1)  # Get the top k experts\n",
    "\n",
    "        # Diversification loss\n",
    "        outputs = []\n",
    "        for i in range(x.shape[0]):  # For each item in the batch\n",
    "            expert_outputs = []\n",
    "            for idx in top_indices[i]:\n",
    "                expert_outputs.append(self.experts[idx](x[i].unsqueeze(0)))\n",
    "            expert_outputs = torch.stack(expert_outputs)\n",
    "            outputs.append((top_weights[i].unsqueeze(-1) * expert_outputs).sum(dim=0))  # Weighted sum of top-k expert outputs\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "        diversification_loss = ((outputs - outputs.mean(dim=0, keepdim=True))**2).sum()\n",
    "\n",
    "        # Dispatching loss\n",
    "        dispatching_loss = 0\n",
    "        for i in range(x.shape[0]):  # For each item in the batch\n",
    "            expert_outputs = []\n",
    "            for idx in top_indices[i]:\n",
    "                expert_output = self.experts[idx](x[i].unsqueeze(0))\n",
    "                expert_outputs.append(expert_output)\n",
    "            expert_outputs = torch.stack(expert_outputs)\n",
    "            target_output = target[i].unsqueeze(0).expand_as(expert_outputs)\n",
    "            dispatching_loss += (top_weights[i].unsqueeze(-1) * (expert_outputs - target_output)**2).sum()\n",
    "\n",
    "        dispatching_loss = dispatching_loss / x.shape[0]  # Average over the batch size\n",
    "\n",
    "        return alpha * diversification_loss + (1-alpha) * dispatching_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b4c88f",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:#3bbfa0; font-size:1.4em;\">**3. Sample Results:**</span><br>\n",
    "\n",
    "Weight (EDD) 0.1 Alpha (EDD) 0.1 Samples (Smoothing) n-smples10 Noise (Smoothing) 0.05\n",
    "<img src=\"./assets/img/Weight (EDD) 0.1 Alpha (EDD) 0.1 Samples (Smoothing) n-smples10 Noise (Smoothing) 0.05.gif\">\n",
    "\n",
    "<br><br>\n",
    "Weight (EDD) 0.5 Alpha (EDD) 0.7 Samples (Smoothing) n-smples30 Noise (Smoothing) 0.05\n",
    "<img src=\"./assets/img/Weight (EDD) 0.5 Alpha (EDD) 0.7 Samples (Smoothing) n-smples30 Noise (Smoothing) 0.05.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49db16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2decb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd3fb57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2192052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
