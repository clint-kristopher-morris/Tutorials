{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2c188f",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3bbfa0; font-size:2em;\">*If You're Always Using the 'Same' Padding, You're Probably Doing It Wrong*</span><br><span style=\"color:lightgray; font-size:2em;\">*What is Padding Really and How to Select it*</span>\n",
    "<br>\n",
    "<img src=\"./asset/img/padding.gif\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cd5b030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.7949,  1.0612, -0.7323, -0.2455, -0.1001],\n",
       "          [-2.7019,  0.3012,  0.0639, -1.2704, -0.3334],\n",
       "          [ 0.2886, -0.3720, -0.1025,  0.5043, -0.7222],\n",
       "          [-0.5054,  0.8228, -1.0236, -1.2244, -1.9280],\n",
       "          [ 0.3289, -0.3378,  0.7619,  0.6806, -0.5579]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Initialize a random tensor\n",
    "x = torch.randn(1, 1, 5, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de81a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5859, -0.0851, -1.1073, -0.2818, -0.4121],\n",
       "          [-1.4088,  1.0385, -0.3710, -0.8102,  0.0723],\n",
       "          [-0.4765, -0.7008, -0.1255, -1.0180, -1.0310],\n",
       "          [-0.2687,  0.2851, -0.7900, -0.0779, -0.4261],\n",
       "          [-0.4424, -0.0440,  0.2546, -0.0673, -0.9075]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolutional layer with padding\n",
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)\n",
    "out = conv(x)\n",
    "print(out.shape)  # Prints: torch.Size([1, 1, 5, 5])\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14ee62",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3bbfa0; font-size:1.2em;\">*1.1 Constant Padding:*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c88a6d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 7.7000e+01,  7.7000e+01,  7.7000e+01,  7.7000e+01,  7.7000e+01,\n",
       "            7.7000e+01,  7.7000e+01],\n",
       "          [ 7.7000e+01, -7.9491e-01,  1.0612e+00, -7.3232e-01, -2.4549e-01,\n",
       "           -1.0012e-01,  7.7000e+01],\n",
       "          [ 7.7000e+01, -2.7019e+00,  3.0118e-01,  6.3919e-02, -1.2704e+00,\n",
       "           -3.3337e-01,  7.7000e+01],\n",
       "          [ 7.7000e+01,  2.8865e-01, -3.7205e-01, -1.0249e-01,  5.0432e-01,\n",
       "           -7.2220e-01,  7.7000e+01],\n",
       "          [ 7.7000e+01, -5.0544e-01,  8.2278e-01, -1.0236e+00, -1.2244e+00,\n",
       "           -1.9280e+00,  7.7000e+01],\n",
       "          [ 7.7000e+01,  3.2894e-01, -3.3782e-01,  7.6193e-01,  6.8059e-01,\n",
       "           -5.5793e-01,  7.7000e+01],\n",
       "          [ 7.7000e+01,  7.7000e+01,  7.7000e+01,  7.7000e+01,  7.7000e+01,\n",
       "            7.7000e+01,  7.7000e+01]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constant padding\n",
    "constant_pad = nn.ConstantPad2d(padding=1, value=77)\n",
    "out = constant_pad(x)\n",
    "print(out.shape)  # Prints: torch.Size([1, 1, 7, 7])\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae023c",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3bbfa0; font-size:1.2em;\">*1.2 Reflection Padding:*</span>\n",
    "\n",
    "```For example, let's say we have a simple one-dimensional tensor [1, 2, 3] and we want to \n",
    "apply reflection padding of size 2. The reflected tensor would look like this: [3, 2, 1, 2, 3, 2, 1].```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "099bea8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3012, -2.7019,  0.3012,  0.0639, -1.2704, -0.3334, -1.2704],\n",
       "          [ 1.0612, -0.7949,  1.0612, -0.7323, -0.2455, -0.1001, -0.2455],\n",
       "          [ 0.3012, -2.7019,  0.3012,  0.0639, -1.2704, -0.3334, -1.2704],\n",
       "          [-0.3720,  0.2886, -0.3720, -0.1025,  0.5043, -0.7222,  0.5043],\n",
       "          [ 0.8228, -0.5054,  0.8228, -1.0236, -1.2244, -1.9280, -1.2244],\n",
       "          [-0.3378,  0.3289, -0.3378,  0.7619,  0.6806, -0.5579,  0.6806],\n",
       "          [ 0.8228, -0.5054,  0.8228, -1.0236, -1.2244, -1.9280, -1.2244]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reflection padding\n",
    "reflection_pad = nn.ReflectionPad2d(padding=1)\n",
    "out = reflection_pad(x)\n",
    "print(out.shape)  # Prints: torch.Size([1, 1, 7, 7])\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b56b3f1",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3bbfa0; font-size:1.2em;\">*1.3 Replication Padding:*</span>\n",
    "\n",
    "```For example, if we have a simple one-dimensional tensor [1, 2, 3] and we want to apply\n",
    "replication padding of size 2, the padded tensor would look like this: [1, 1, 1, 2, 3, 3, 3]. \n",
    "The values at the beginning and end of the tensor are \"replicated\" to create padding.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae512ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.7949, -0.7949,  1.0612, -0.7323, -0.2455, -0.1001, -0.1001],\n",
       "          [-0.7949, -0.7949,  1.0612, -0.7323, -0.2455, -0.1001, -0.1001],\n",
       "          [-2.7019, -2.7019,  0.3012,  0.0639, -1.2704, -0.3334, -0.3334],\n",
       "          [ 0.2886,  0.2886, -0.3720, -0.1025,  0.5043, -0.7222, -0.7222],\n",
       "          [-0.5054, -0.5054,  0.8228, -1.0236, -1.2244, -1.9280, -1.9280],\n",
       "          [ 0.3289,  0.3289, -0.3378,  0.7619,  0.6806, -0.5579, -0.5579],\n",
       "          [ 0.3289,  0.3289, -0.3378,  0.7619,  0.6806, -0.5579, -0.5579]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replication padding\n",
    "replication_pad = nn.ReplicationPad2d(padding=1)\n",
    "out = replication_pad(x)\n",
    "print(out.shape)  # Prints: torch.Size([1, 1, 7, 7])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178a126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dec440d0",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "\n",
    "### <span style=\"color:#3bbfa0; font-size:1.2em;\">*2.1 Receptive Fields:*</span>\n",
    "____\n",
    "\n",
    "\n",
    "1. Fawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., Webb, G. I., Idoumghar, \n",
    "L., Muller, P.-A., & Petitjean, F. (2019). InceptionTime: Finding AlexNet for Time Series Classification. arXiv. https://doi.org/10.48550/ARXIV.1909.04939\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d448a3d",
   "metadata": {},
   "source": [
    "<img src=\"./asset/img/paper.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179a03b",
   "metadata": {},
   "source": [
    "<img src=\"./asset/img/rf.png\" width=\"800\">\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7bf28f",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<br><br><br>\n",
    "\n",
    "### <span style=\"color:#3bbfa0; font-size:1.2em;\">*2.2 Receptive Fields:*</span>\n",
    "\n",
    "\n",
    "2. Luo, W., Li, Y., Urtasun, R., & Zemel, R. (2017). Understanding the Effective Receptive Field in Deep Convolutional Neural Networks (Version 2). arXiv. https://doi.org/10.48550/ARXIV.1701.04128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d0981",
   "metadata": {},
   "source": [
    "<img src=\"./asset/img/paper2.png\" width=\"970\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d11cd5",
   "metadata": {},
   "source": [
    "1. **Increasing the Receptive Field Size**: The authors suggest that subsampling and dilated convolutions can be effective ways to increase the receptive field size quickly. This can help to ensure that the network is able to capture more contextual information from the input.\n",
    "\n",
    "2. **Modifying Network Architecture**: The authors also suggest that modifications to the network architecture, such as the use of skip connections, can help to control the size of the ERF. They note that skip connections tend to make the ERF smaller, which can be beneficial in certain contexts.\n",
    "\n",
    "3. **Adjusting Training Techniques**: The authors observed that the ERF changes during the training of deep CNNs on real datasets. They found that as the network learns, the ERF gets bigger, and at the end of training, it is significantly larger than the initial ERF. This suggests that adjusting training techniques could also be a way to manage the size of the ERF.\n",
    "\n",
    "4. **Balancing the Receptive Field and Resolution**: The authors suggest that there is a trade-off between the size of the receptive field and the resolution of the feature maps. They propose that this trade-off should be carefully considered when designing network architectures and training techniques.\n",
    "\n",
    "5. **Understanding the Impact of Nonlinear Activations**: The authors also discuss the effect of nonlinear activations on the ERF. Understanding these effects can help to design more effective network architectures and training techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c345c6d",
   "metadata": {},
   "source": [
    "<img src=\"./asset/img/activations.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b23d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5389f94e",
   "metadata": {},
   "source": [
    "<img src=\"./asset/img/c10.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6bcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
